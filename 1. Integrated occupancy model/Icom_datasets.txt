

model{

  # Priors ------------------------------------------------------------------
  
  # Single occurrence parameters 
  int.beta.mean ~ dunif(0, 1) # overall  
  beta.0.mean <- logit(int.beta.mean)
  beta.1.mean ~ dnorm(0, 0.368) # linear access
  beta.2.mean ~ dnorm(0, 0.368) # quadratic access
  beta.3.mean ~ dnorm(0, 0.368) # linear tree height
  beta.4.mean ~ dnorm(0, 0.368) # quadratic tree height
  beta.5.mean ~ dnorm(0, 0.368) # linear tree variance
  
  #-----------------------------------
  # Detection parameters specific to dataset 
  
  # overall SMART detection--------------------
  # Intercept = Slope Habitat Type (effect parameterization)
  # First we specify one hyperparameter for each habitat type
    for(h in 1:n.hab.sm){
       gamma.2.0.mean[h] ~ dnorm(0, 0.368)
       tau.gamma.2.0[h] ~ dgamma(0.1, 0.1) #add tau in the loop
    }

  gamma.2.1.mean ~ dnorm(0, 0.368) # TRI
  gamma.2.2.mean ~ dnorm(0, 0.368) # WP
       
       
  # Overall SWTS detection--------------------
  # Intercept = Slope Habitat Type (effect parameterization)
  # First we specify one hyperparameter for each habitat type
    for(h in 1:n.hab.tr){
       gamma.1.0.mean[h] ~ dnorm(0, 0.368)
       tau.gamma.1.0[h] ~ dgamma(0.1, 0.1) #add tau in the loop
    }
      
  gamma.1.1.mean ~ dnorm(0, 0.368) # TRI
  gamma.1.2.mean ~ dnorm(0, 0.368) # Transect length


  # Overall CT detection--------------------
  # Intercept = Slope Camera Type (effect parameterization)
  # First we specify one hyperparameter for each camera type
    for(h in 1:n.type.ct){
       alpha.0.mean[h] ~ dnorm(0, 0.368)
       tau.alpha.0[h] ~ dgamma(0.1, 0.1) #add tau in the loop
    }
       
  alpha.1.mean ~ dnorm(0, 0.368) # TRI
  alpha.2.mean ~ dnorm(0, 0.368) # Trap days
  
  
  # Hyperprior for half-Cauchy scale parameter for process model ---------
  # Half-cauchy priors implemented as weakly informative priors to shrink parameter estimates 
  # towards zero and mitigate variance inflation (following Broms et al. 2016; Ecology)

  xi.tau <- pow(xi.sd, -2)
  xi.sd ~ dunif(0, 10) 
     
  #-----------------------------------
  # Precision Parameters --------------
  tau.beta.0 ~ dgamma(0.1, 0.1)
  tau.beta.1 ~ dgamma(0.1, 0.1)
  tau.beta.2 ~ dgamma(0.1, 0.1)
  tau.beta.3 ~ dgamma(0.1, 0.1)
  tau.beta.4 ~ dgamma(0.1, 0.1)
  tau.beta.5 ~ dgamma(0.1, 0.1)

  tau.gamma.2.1 ~ dgamma(0.1, 0.1) #patrol
  tau.gamma.2.2 ~ dgamma(0.1, 0.1)
  
  tau.gamma.1.1 ~ dgamma(0.1, 0.1) #transect
  tau.gamma.1.2 ~ dgamma(0.1, 0.1)
  
  tau.alpha.1 ~ dgamma(0.1, 0.1) #camera
  tau.alpha.2 ~ dgamma(0.1, 0.1)
  
  #-----------------------------------
  # Species-specific coefficients -----------------------------------------
  for (i in 1:I) {
    beta.0[i] ~ dnorm(beta.0.mean, tau.beta.0)
    beta.1[i] ~ dnorm(beta.1.mean, tau.beta.1)
    beta.2[i] ~ dnorm(beta.2.mean, tau.beta.2)
    beta.3[i] ~ dnorm(beta.3.mean, tau.beta.3)
    beta.4[i] ~ dnorm(beta.4.mean, tau.beta.4)
    beta.5[i] ~ dnorm(beta.5.mean, tau.beta.5)
    
    
    # Year-related Random effect
    for(year in 1:n.year){
        eps[i,year] ~ dnorm(0, eps.tau[i])
        }

      eps.tau[i] ~ dgamma(0.5, 0.5)
      xi[i] ~ dnorm(0, xi.tau)
      sigma.cauchy[i] <- abs(xi[i]) / sqrt(eps.tau[i])
  
  } #i

  
  #SMART detection
  for (i in 1:I) {
    gamma.2.1[i] ~ dnorm(gamma.2.1.mean, tau.gamma.2.1)
    gamma.2.2[i] ~ dnorm(gamma.2.2.mean, tau.gamma.2.2)
    
    # For the sums-to-zero approach we need to specify a temporary parameter drawn from the hyperparameters
    # Then we subtract the mean of all categories from this
    for(h in 1:n.hab.sm){
        gamma.2.0[i,h] ~ dnorm(gamma.2.0.mean[h], tau.gamma.2.0[h])
      }
  } #i
  
  #SWTS detection
  for (i in 1:I) {
    gamma.1.1[i] ~ dnorm(gamma.1.1.mean, tau.gamma.1.1)
    gamma.1.2[i] ~ dnorm(gamma.1.2.mean, tau.gamma.1.2)   
    
    # For the sums-to-zero approach we need to specify a temporary parameter drawn from the hyperparameters
    # Then we subtract the mean of all categories from this
    for(h in 1:n.hab.tr){
        gamma.1.0[i,h] ~ dnorm(gamma.1.0.mean[h], tau.gamma.1.0[h])
      }
  } # i
  
    #CT detection
  for (i in 1:I) {
    alpha.1[i] ~ dnorm(alpha.1.mean, tau.alpha.1)
    alpha.2[i] ~ dnorm(alpha.2.mean, tau.alpha.2)
    
    # For the sums-to-zero approach we need to specify a temporary parameter drawn from the hyperparameters
    # Then we subtract the mean of all categories from this
    for(h in 1:n.type.ct){
        alpha.0[i,h] ~ dnorm(alpha.0.mean[h], tau.alpha.0[h])
      }
  } # i
  
  
  #-----------------------------------
  # Process Models and Likelihoods ----------------------------------------
  
  # SMART
  for (j in 1:J.sm) {
    for (i in 1:I) {
      logit(psi.sm[i, j]) <- beta.0[i] + 
        beta.1[i] * ACCESS.sm[j] + beta.2[i] * pow(ACCESS.sm[j], 2) +
        beta.3[i] * TREE.sm[j] + beta.4[i] * pow(TREE.sm[j], 2) +
        beta.5[i] * CANOPY.sm[j] + 
        xi[i]*eps[i,YEAR.sm[j]] #we added year random effect
      
      z.sm[i, j] ~ dbern(psi.sm[i, j])
    } # i
  } # j
  
  
  # SWTS
  for (j in 1:J.tr) {
    for (i in 1:I) {
       logit(psi.tr[i, j]) <- beta.0[i] + 
                             beta.1[i] * ACCESS.tr[j] + beta.2[i] * pow(ACCESS.tr[j], 2) +
                             beta.3[i] * TREE.tr[j] + beta.4[i] * pow(TREE.tr[j], 2) +
                             beta.5[i] * CANOPY.tr[j] + 
                             xi[i]*eps[i,YEAR.tr[j]] #we added year random effect
      
      z.tr[i, j] ~ dbern(psi.tr[i, j])
    } # i
  } # j
  
  # CT
  for (j in 1:J.ct) {
    for (i in 1:I) {
      logit(psi.ct[i, j]) <- beta.0[i] + 
        beta.1[i] * ACCESS.ct[j] + beta.2[i] * pow(ACCESS.ct[j], 2) +
        beta.3[i] * TREE.ct[j] + beta.4[i] * pow(TREE.ct[j], 2) +
        beta.5[i] * CANOPY.ct[j] + 
        xi[i]*eps[i,YEAR.ct[j]] #we added year random effect
        
      z.ct[i, j] ~ dbern(psi.ct[i, j])
    } # i
  } # j

#-------------------------------------------------------------------------------  
  # SMART Leuser Data ----------------------------------------------------
  # Data are stacked in a single vector as opposed to a multi-dimensional 
  # array to improve computational performance. 
  for (i in 1:n.vals.sm) {
    logit(p.sm[i]) <- gamma.2.0[sp.indx.sm[i], hab.sm[i]] +
      gamma.2.1[sp.indx.sm[i]] * TRI.sm[i] +
      gamma.2.2[sp.indx.sm[i]] * wp.sm[i] 
    
    dsm[i] ~ dbern(p.sm[i] * z.sm[sp.indx.sm[i], site.sm[i]]) #*y=det/non
    
    # Replicate of data for Bayesian p-value
    dsm.rep[i] ~ dbern(p.sm[i] * z.sm[sp.indx.sm[i], site.sm[i]]) #expesmed
    E.sm[i] <- p.sm[i] * z.sm[sp.indx.sm[i], site.sm[i]]
    TRI.sm[i] ~ dnorm(0, 1)
  } # i
  
  
  # Transect Leuser Data ----------------------------------------------------
  # Data are stacked in a single vector as opposed to a multi-dimensional 
  # array to improve computational performance. 
  # Create two loops (1st replicate and subsequent replicates)
  
  for(k1 in 1:n.vals.tr) {                                                                                 ## Nick comment: looping over all sites where segment == 1

      TRI.tr[k1] ~ dnorm(0, 1)                                                                       ## Nick comment: What is TRI.tr? It looks like a covariate drawn from a normal distribution - what does this account for?
      logit(p.tr[k1]) <- gamma.1.0[sp.indx.tr[k1], hab.tr[k1]] + 
                         gamma.1.1[sp.indx.tr[k1]] * TRI.tr[k1] +
                         gamma.1.2[sp.indx.tr[k1]] * leng.tr[k1] 
           
          dtr[k1] ~ dbern(p.tr[k1] * z.tr[sp.indx.tr[k1], site.tr[k1]])
          
          # Replicate of data for Bayesian p-value
          dtr.rep[k1] ~ dbern(p.tr[k1] * z.tr[sp.indx.tr[k1], site.tr[k1]])
          E.tr[k1] <- p.tr[k1] * z.tr[sp.indx.tr[k1], site.tr[k1]]
  } #k1


  # Camera Trap Leuser Data ----------------------------------------------------
  # Data are stacked in a single vector as opposed to a multi-dimensional 
  # array to improve computational performance. 
  for (i in 1:n.vals.ct) {
    logit(p.ct[i]) <- alpha.0[sp.indx.ct[i], type.ct[i]] +
      alpha.1[sp.indx.ct[i]] * TRI.ct[i] +
      alpha.2[sp.indx.ct[i]] * trap.ct[i] 
    
    y[i] ~ dbern(p.ct[i] * z.ct[sp.indx.ct[i], site.ct[i]]) #*y=det/non
    
    # Replicate of data for Bayesian p-value
    y.rep[i] ~ dbern(p.ct[i] * z.ct[sp.indx.ct[i], site.ct[i]]) #expected
    E.ct[i] <- p.ct[i] * z.ct[sp.indx.ct[i], site.ct[i]]
    TRI.ct[i] ~ dnorm(0, 1)
  } # i
  
#----------------------------------------------------------------------------  
    # Compute Bayesian P-value GoF Assessment -------------------------------
    for (i in 1:I) {
      dsm.grouped[i] <- sum(dsm[low.bp.dsm[i]:high.bp.dsm[i]])
      dsm.rep.grouped[i] <- sum(dsm.rep[low.bp.dsm[i]:high.bp.dsm[i]])
      E.grouped.dsm[i] <- sum(E.sm[low.bp.dsm[i]:high.bp.dsm[i]])
      x2.dsm[i] <- pow((dsm.grouped[i] - E.grouped.dsm[i]), 2) / (E.grouped.dsm[i] + e) #equal to chi2.actual
      x2.dsm.rep[i] <- pow((dsm.rep.grouped[i] - E.grouped.dsm[i]), 2) / (E.grouped.dsm[i] + e) #equal to chi2.sim
      
      dtr.grouped[i] <- sum(dtr[low.bp.dtr[i]:high.bp.dtr[i]])
      dtr.rep.grouped[i] <- sum(dtr.rep[low.bp.dtr[i]:high.bp.dtr[i]])
      E.grouped.dtr[i] <- sum(E.tr[low.bp.dtr[i]:high.bp.dtr[i]])
      x2.dtr[i] <- pow((dtr.grouped[i] - E.grouped.dtr[i]), 2) / (E.grouped.dtr[i] + e) #equal to chi2.actual
      x2.dtr.rep[i] <- pow((dtr.rep.grouped[i] - E.grouped.dtr[i]), 2) / (E.grouped.dtr[i] + e) #equal to chi2.sim
    
      y.grouped[i] <- sum(y[low.bp.y[i]:high.bp.y[i]])
      y.rep.grouped[i] <- sum(y.rep[low.bp.y[i]:high.bp.y[i]])
      E.grouped.y[i] <- sum(E.ct[low.bp.y[i]:high.bp.y[i]])
      x2.y[i] <- pow((y.grouped[i] - E.grouped.y[i]), 2) / (E.grouped.y[i] + e) #equal to chi2.actual
      x2.y.rep[i] <- pow((y.rep.grouped[i] - E.grouped.y[i]), 2) / (E.grouped.y[i] + e) #equal to chi2.sim
    } #i 
  
    # Add up Chi-square discrepancies]
    
    fit.actual.sm <- sum(x2.dsm[1:I]) 
    fit.sim.sm <- sum(x2.dsm.rep[1:I])
    
    fit.actual.tr <- sum(x2.dtr[1:I]) 
    fit.sim.tr <- sum(x2.dtr.rep[1:I])
    
    fit.actual.ct <- sum(x2.y[1:I]) 
    fit.sim.ct <- sum(x2.y.rep[1:I])
    
    # Calculate overall chi-squared discrepency measure
    #==================================================
    c.hat.sm <- fit.actual.sm/fit.sim.sm
    c.hat.tr <- fit.actual.tr/fit.sim.tr
    c.hat.ct <- fit.actual.ct/fit.sim.ct
    
    bpv.sm <- step(fit.sim.sm - fit.actual.sm)
    bpv.tr <- step(fit.sim.tr - fit.actual.tr)
    bpv.ct <- step(fit.sim.ct - fit.actual.ct) 
    
  
    # Derived quantities
    # Number of occupied sites per SPECIES (perhaps per data source)
    #=========================
    for(i in 1:I) {
        Nocc.fs.sm[i] <- sum(z.sm[i,])
        Nocc.fs.tr[i] <- sum(z.tr[i,])
        Nocc.fs.ct[i] <- sum(z.ct[i,])
    }
    
    
    # Average occupancy for each species
    #=========================================
    for(i in 1:I) {
        mean.psi.sm[i] <- mean(psi.sm[i,])
        mean.psi.tr[i] <- mean(psi.tr[i,])
        mean.psi.ct[i] <- mean(psi.ct[i,])
    }  
    
    # Psi spatial prediction across Leuser Landscape
    #=========================================
    for (i in 1:I) {  
      for (k in 1:nGrid) {
      logit(Occu.pred[k,i]) <- beta.0[i] + 
        beta.1[i] * ACCESS.LEUSER[k] + beta.2[i] * pow(ACCESS.LEUSER[k], 2) +
        beta.3[i] * TREE.LEUSER[k] + beta.4[i] * pow(TREE.LEUSER[k], 2) +
        beta.5[i] * CANOPY.LEUSER[k] 
      
      z.leuser[k,i] ~ dbern(Occu.pred[k, i])
      } #k
    } #i
        
}


